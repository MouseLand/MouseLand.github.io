---
title: "Facemap: a framework for modeling neural activity based on orofacial tracking"
description: |
  Analysis tool for tracking mouse face keypoints and relating them to large-scale neural activity, using convolutional neural networks.
categories:
  - Behavior
  - Thousands of neurons
  - Machine learning
  - Tools
date: Nov 2023
date-format: MMM YYYY
author: Atika Syeda, Lin Zhong, Renee Tung, Will Long, Marius Pachitariu<sup>†</sup>, Carsen Stringer<sup>†</sup>
toc: false
image: https://raw.githubusercontent.com/MouseLand/facemap/main/facemap/mouse.png
image-alt: facemap
---

<p style="font-size: smaller">
Recent studies in mice have shown that orofacial behaviors drive a large fraction of neural activity across the brain. To understand the nature and function of these signals, we need better computational models to characterize the behaviors and relate them to neural activity. Here we developed Facemap, a framework consisting of a keypoint tracker and a deep neural network encoder for predicting neural activity. Our algorithm for tracking mouse orofacial behaviors was more accurate than existing pose estimation tools, while the processing speed was several times faster, making it a powerful tool for real-time experimental interventions. The Facemap tracker was easy to adapt to data from new labs, requiring as few as 10 annotated frames for near-optimal performance. We used the keypoints as inputs to a deep neural network which predicts the activity of ~50,000 simultaneously-recorded neurons and, in visual cortex, we doubled the amount of explained variance compared to previous methods. Using this model, we found that the neuronal activity clusters that were well predicted from behavior were more spatially spread out across cortex. We also found that the deep behavioral features from the model had stereotypical, sequential dynamics that were not reversible in time. In summary, Facemap provides a stepping stone toward understanding the function of the brain-wide neural signals and their relation to behavior. </p>

<a href="https://www.nature.com/articles/s41593-023-01490-6">article</a> | 
<a href="https://www.biorxiv.org/content/10.1101/2022.11.03.515121v1">preprint</a> | 
<a href="https://github.com/MouseLand/facemap">code</a> |
<a href="https://www.nature.com/articles/s41592-023-02154-w">Nature Methods highlight</a> |
<a href="https://www.janelia.org/news/reading-the-mouse-mind-from-its-face-new-tool-decodes-neural-activity-using-facial-movements">news article</a> |
<a href="https://www.youtube.com/watch?v=4fIXfSBKWu8">talk</a> |
<a href="https://twitter.com/Atika_Ibrahim/status/1588885329951367168">tweeprint</a></h2><br>

<hr><br>

Tutorial:

<iframe width="560" height="315" src="https://www.youtube.com/embed/aO_kXkOuadg?si=88yiUBJoMDT46S4_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

GUI:

<img src="https://raw.githubusercontent.com/MouseLand/facemap/main/figs/facemap.gif" width="100%">
<br>

