[
  {
    "objectID": "code_and_data.html",
    "href": "code_and_data.html",
    "title": "Code & datasets",
    "section": "",
    "text": "We develop and maintain several analysis frameworks for neuroscience and biology more generally:\n\nCellpose is a generalist, deep learning-based segmentation algorithm written in Python which can precisely segment cells from a wide range of image types – try it out on your own data easily on our website: https://www.cellpose.org. Cellpose can be applied to 2D and 3D imaging data without requiring 3D-labelled data. It has an easy-to-use graphical user interface for manual labeling and for curation of the automated results. It can also be used to train new models on user data, and for denoising, deblurring, or upsampling images. Software developers have integrated Cellpose into their own image processing software, such as CellProfiler, ImagePy, ImJoy, aPeer, Napari.\n\n\n\n\n\n \nKilosort is the most popular spike sorting tool for Neuropixels probes, and it also works on other probes. The software is GPU-accelerated to enable fast and accurate spike sorting. Kilosort first detects spikes, then uses these spikes to perform drift correction – an important step in most recording setups. Next it clusters the spikes using novel graph-based clustering techniques. Kilosort4 is implemented in python with an easy-to-use GUI.\n\n\n\n\nSuite2p is a fast, accurate and complete pipeline written in Python that registers raw movies, detects active cells, extracts their calcium traces and infers their spike times. Suite2p runs on standard workstations, operates faster than real time, and recovers ~2 times more cells than the previous state-of-the-art methods. Its low computational load allows routine detection of ~25,000 cells simultaneously from recordings taken with standard two-photon resonant-scanning microscopes. In addition to its ability to detect cell somas, the detection algorithm can detect axonal segments, boutons, dendrites, and spines. Suite2p has an extensive GUI which allows the user to explore their data. Software developers have integrated Suite2p into their packages, such as those for multi-day cell alignment and photostimulation experiments.\n\n\n\n\n\nFacemap is a tool for extracting behavioral features from mouse face videos and using them to predict neural activity. Facemap can be used to extract a 500-dimensional summary of the motor actions visible on the mouse’s face by applying singular value decomposition (SVD) to the facial motion. It can also be used to track keypoints on the mouse face, using a state-of-the-art deep neural network. Facemap also includes a 1D convolutional neural network to predict neural activity from keypoints or SVDs, which is two times more accurate than previous approaches. Facemap’s GUI enables movie playback with behavioral feature tracking and neural activity.\n\n\n\n\nRastermap is a tool for visualizing large-scale neural activity by applying a one-dimensional manifold embedding. To preserve both local and global structure, Rastermap combines manifold discovery and clustering. To capture temporal relationships among clusters, we compute not just the instantaneous correlations between cluster activities but also the cross-correlations of the clusters. Next we sort these clusters to optimize local and global distance preservation. Then the sorting is upsampled so that neurons can be assigned to their most correlated place in the one-dimensional embedding. This enables Rastermap to find sequences in visual cortical neural activity evoked by virtual reality corridors, which t-SNE and UMAP cannot do, and also Rastermap outperforms these algorithms on structure preservation benchmarks. Rastermap can be run in a jupyter-notebook, on the command line, in the provided GUI, or inside Suite2p to explore the spatial relationships among neurons identified to have similar activity patterns."
  },
  {
    "objectID": "code_and_data.html#code",
    "href": "code_and_data.html#code",
    "title": "Code & datasets",
    "section": "",
    "text": "We develop and maintain several analysis frameworks for neuroscience and biology more generally:\n\nCellpose is a generalist, deep learning-based segmentation algorithm written in Python which can precisely segment cells from a wide range of image types – try it out on your own data easily on our website: https://www.cellpose.org. Cellpose can be applied to 2D and 3D imaging data without requiring 3D-labelled data. It has an easy-to-use graphical user interface for manual labeling and for curation of the automated results. It can also be used to train new models on user data, and for denoising, deblurring, or upsampling images. Software developers have integrated Cellpose into their own image processing software, such as CellProfiler, ImagePy, ImJoy, aPeer, Napari.\n\n\n\n\n\n \nKilosort is the most popular spike sorting tool for Neuropixels probes, and it also works on other probes. The software is GPU-accelerated to enable fast and accurate spike sorting. Kilosort first detects spikes, then uses these spikes to perform drift correction – an important step in most recording setups. Next it clusters the spikes using novel graph-based clustering techniques. Kilosort4 is implemented in python with an easy-to-use GUI.\n\n\n\n\nSuite2p is a fast, accurate and complete pipeline written in Python that registers raw movies, detects active cells, extracts their calcium traces and infers their spike times. Suite2p runs on standard workstations, operates faster than real time, and recovers ~2 times more cells than the previous state-of-the-art methods. Its low computational load allows routine detection of ~25,000 cells simultaneously from recordings taken with standard two-photon resonant-scanning microscopes. In addition to its ability to detect cell somas, the detection algorithm can detect axonal segments, boutons, dendrites, and spines. Suite2p has an extensive GUI which allows the user to explore their data. Software developers have integrated Suite2p into their packages, such as those for multi-day cell alignment and photostimulation experiments.\n\n\n\n\n\nFacemap is a tool for extracting behavioral features from mouse face videos and using them to predict neural activity. Facemap can be used to extract a 500-dimensional summary of the motor actions visible on the mouse’s face by applying singular value decomposition (SVD) to the facial motion. It can also be used to track keypoints on the mouse face, using a state-of-the-art deep neural network. Facemap also includes a 1D convolutional neural network to predict neural activity from keypoints or SVDs, which is two times more accurate than previous approaches. Facemap’s GUI enables movie playback with behavioral feature tracking and neural activity.\n\n\n\n\nRastermap is a tool for visualizing large-scale neural activity by applying a one-dimensional manifold embedding. To preserve both local and global structure, Rastermap combines manifold discovery and clustering. To capture temporal relationships among clusters, we compute not just the instantaneous correlations between cluster activities but also the cross-correlations of the clusters. Next we sort these clusters to optimize local and global distance preservation. Then the sorting is upsampled so that neurons can be assigned to their most correlated place in the one-dimensional embedding. This enables Rastermap to find sequences in visual cortical neural activity evoked by virtual reality corridors, which t-SNE and UMAP cannot do, and also Rastermap outperforms these algorithms on structure preservation benchmarks. Rastermap can be run in a jupyter-notebook, on the command line, in the provided GUI, or inside Suite2p to explore the spatial relationships among neurons identified to have similar activity patterns."
  },
  {
    "objectID": "code_and_data.html#datasets",
    "href": "code_and_data.html#datasets",
    "title": "Code & datasets",
    "section": "Datasets",
    "text": "Datasets\nWe share our large-scale recordings of mouse cortex on figshare:\n\nVisual response dataset (Du et al 2025): recordings of 29,000 neurons in mouse primary visual cortex in response to up to 65,000 natural images; analysis code\nVisual learning dataset (Zhong et al 2025): recordings of 50,000+ neurons simultaneously in mouse visual cortex as mice undergo unsupervised and task learning in virtual reality; analysis code\nFacemap dataset (Syeda et al 2024): spontaneous neural activity from 50,000+ neurons in mouse visual cortex and sensorimotor cortex, simultaneous face camera recordings, and keypoint tracking training set\nNeural responses to oriented stimuli (Stringer et al 2021): Responses of 20,000+ neurons in mouse primary visual cortex and higher order visual cortex; analysis code\nSpontaneous neural activity in V1 (Stringer, Pachitariu et al 2019): Recordings of 10,000 neurons in visual cortex during spontaneous behaviors; analysis code\nEight-probe Neuropixels recordings during spontaneous behaviors by Nicholas Steinmetz, from Stringer, Pachitariu et al 2019\nNeural responses to natural images (Stringer, Pachitariu et al 2019): Recordings of 10,000 neurons in primary visual cortex in response to 2,800 natural images; analysis code\nV1 responses to drifting gratings (Pachitariu et al 2018): Responses of 10,000 neurons in mouse V1 during drifting gratings\n\nWe also shared the training data for the Cellpose algorithm: 70,000 segmented cells + other objects.\nAnd we shared the simulations from the Kilosort4 paper: ephys simulations."
  },
  {
    "objectID": "research/posts/cellpose.html",
    "href": "research/posts/cellpose.html",
    "title": "Cellpose: a generalist algorithm for cellular segmentation",
    "section": "",
    "text": "Abstract\n\n\n\nMany biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.\n\n\npaper | talk | news article | preprint | code | original tweeprint\n\n\nThread:\n\nReleasing Cellpose, a generalist algorithm for cellular segmentation. Try it now directly on the website www.cellpose.org, or install the GUI with pip install cellpose:\n\n\n\n\n\n\n\nWe developed Cellpose as a generalist algorithm, because many small and big problems in biology require cell segmentation, and there just isn’t enough time to write a new pipeline for every type of data.\n\n\n\n\n\n\nWe trained Cellpose on a diverse set of 608 images, collected and segmented by us. See the t-SNE plot of image styles below.\n\n\n\n\n\n\nOut-of-the box, Cellpose can segment a large variety of images from different types of microscopy, different tissues and different stains or fluorescent tags. It can even segment rocks, jellyfish and sea urchins.\n\n\n\n\n\n\nThe GUI lets you manually segment your own images at a speed of 300-600 objects per hour. Cellpose doesn’t need super precise outlines.\n\n\n\n\n\n\n\nSend us your manual segmentations and we’ll include them in the next Cellpose release, making the model better for yourself and everyone else! P.S. thanks for sending us your segmentations, they are now in the “cyto2” model!\n\n\n\n\n\n\nAlso you can perform 3D segmentation without 3D training data!\n\n\n\n\n\n\nCheck out the [paper] for much more: nucleus segmentation, comparisons to previous state-of-the-art methods, a cell size prediction network etc.\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/suite2p.html",
    "href": "research/posts/suite2p.html",
    "title": "Suite2p: beyond 10,000 neurons with standard two-photon microscopy",
    "section": "",
    "text": "Two-photon microscopy of calcium-dependent sensors has enabled unprecedented recordings from vast populations of neurons. While the sensors and microscopes have matured over several generations of development, computational methods to process the resulting movies remain inefficient and can give results that are hard to interpret. Here we introduce Suite2p: a fast, accurate and complete pipeline that registers raw movies, detects active cells, extracts their calcium traces and infers their spike times. Suite2p runs on standard workstations, operates faster than real time, and recovers ~2 times more cells than the previous state-of-the-art method. Its low computational load allows routine detection of ~10,000 cells simultaneously with standard two-photon resonant-scanning microscopes. Recordings at this scale promise to reveal the fine structure of activity in large populations of neurons or large populations of subcellular structures such as synaptic boutons.\n\ntalk | code\n\n\n\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/kilosort.html",
    "href": "research/posts/kilosort.html",
    "title": "Spike sorting with Kilosort4",
    "section": "",
    "text": "Spike sorting is the computational process of extracting the firing times of single neurons from recordings of local electrical fields. This is an important but hard problem in neuroscience, made complicated by the nonstationarity of the recordings and the dense overlap in electrical fields between nearby neurons. To address the spike-sorting problem, we have been openly developing the Kilosort framework. Here we describe the various algorithmic steps introduced in different versions of Kilosort. We also report the development of Kilosort4, a version with substantially improved performance due to clustering algorithms inspired by graph-based approaches. To test the performance of Kilosort, we developed a realistic simulation framework that uses densely sampled electrical fields from real experiments to generate nonstationary spike waveforms and realistic noise. We found that nearly all versions of Kilosort outperformed other algorithms on a variety of simulated conditions and that Kilosort4 performed best in all cases, correctly identifying even neurons with low amplitudes and small spatial extents in high drift conditions.\n\npaper | code | talk | HHMI news article | preprint\n\n\n\n\nGraphical user interface:\n\n\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/critical_init.html",
    "href": "research/posts/critical_init.html",
    "title": "A critical initialization for biological neural networks",
    "section": "",
    "text": "Abstract\n\n\n\nArtificial neural networks learn faster if they are initialized well. Good initializations can generate high-dimensional macroscopic dynamics with long timescales. It is not known if biological neural networks have similar properties. Here we show that the eigenvalue spectrum and dynamical properties of large-scale neural recordings in mice (two-photon and electrophysiology) are similar to those produced by linear dynamics governed by a random symmetric matrix that is critically normalized. An exception was hippocampal area CA1: population activity in this area resembled an efficient, uncorrelated neural code, which may be optimized for information storage capacity. Global emergent activity modes persisted in simulations with sparse, clustered or spatial connectivity. We hypothesize that the spontaneous neural activity reflects a critical initialization of whole-brain neural circuits that is optimized for learning time-dependent tasks.\n\n\npreprint | code | bluesky thread\n\nThread:\n\nWhat if… spontaneous neural activity 🧠 reflects the baseline rumblings of a brainwide dynamical system initialized for learning? We find that the rumblings have macroscopic properties like those emerging from linear symmetric, critical systems 🧵 #neuroscience #neuroAI\n\n\n\n\n\n\nLong timescales and large principal components (PCs) can be produced by a dynamical system with random connectivity and independent stochastic inputs, if the connectivity matrix is critically-normalized.\n\n\n\n\n\n\nFurthermore, the principal components in the model decay as a power-law, a phenomenon we have previously reported in large-scale neural recordings: https://www.science.org/doi/10.1126/science.aav7893; https://www.nature.com/articles/s41586-019-1346-5\nIn V1 and brainwide ephys recordings we observed that the PC variances decayed as a power-law with exponents of 0.7-0.85, consistent with symmetric, critically-normalized simulations.\n\n\n\n\n\n\nBut in hippocampus, we observed exponents around 0.5 that did not change after shuffling, suggesting that hippocampal activity is closer to completely independent neurons.\n\n\n\n\n\n\nThe model also predicted that higher PCs have longer timescales, which was true in the data.\n\n\n\n\n\n\nAn estimate of the dynamics matrix of the data (using DMD) revealed mostly real eigenvalues, further supporting symmetric dynamics.\n\n\n\n\n\n\nGlobal emergent activity modes persisted in simulations with sparse, clustered or spatial connectivity.\n\n\n\n\n\n\nSimulations with spatial connectivity replicated several of the properties we observed in the neural recordings, such as a spatial dependence of top correlated neuron pairs, and top PCs which were globally spread across cortex.\n\n\n\n\n\n\nWe hypothesize that the spontaneous neural activity reflects a critical initialization of whole-brain neural circuits that is optimized for learning tasks that are time-dependent and working-memory dependent. More details in the paper by @marius10p.bsky.social.\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/cellpose2.html",
    "href": "research/posts/cellpose2.html",
    "title": "Cellpose 2.0: how to train your own model",
    "section": "",
    "text": "Abstract\n\n\n\nPretrained neural network models for biological segmentation can provide good out-of-the-box results for many image types. However, such models do not allow users to adapt the segmentation style to their specific needs and can perform suboptimally for test images that are very different from the training images. Here we introduce Cellpose 2.0, a new package that includes an ensemble of diverse pretrained models as well as a human-in-the-loop pipeline for rapid prototyping of new custom models. We show that models pretrained on the Cellpose dataset can be fine-tuned with only 500–1,000 user-annotated regions of interest (ROI) to perform nearly as well as models trained on entire datasets with up to 200,000 ROI. A human-in-the-loop approach further reduced the required user annotation to 100–200 ROI, while maintaining high-quality segmentations. We provide software tools such as an annotation graphical user interface, a model zoo and a human-in-the-loop pipeline to facilitate the adoption of Cellpose 2.0.\n\n\npaper | talk | tutorial | news & views | preprint | code | original tweeprint\n\nThis is an upgrade to Cellpose; if you’re unfamiliar with Cellpose, check it out here.\nCellpose 2.0 thread:\n\nYou can now train your own state-of-the-art models in less than 1 hour, all from the GUI. Massive improvements for some images!\n\n\n\n\n\n\nCellpose 1.0 is great, but we were getting reports of imperfect segmentations on some image categories, e.g. on some new large-scale datasets, like TissueNet and LiveCell (see the two images above).\nAs we dove into these datasets, we realized that different people just segment cells in different ways. Here are some representative examples from different datasets:\n\n\n\n\n\nNotice how in different images more or less of the cytoplasm is segmented? Or how nuclei may or may not be segmented when they don’t have cytoplasm? Or how too dense regions are sometimes not annotated? The list goes on and on.\n\nIt’s impossible for a single model like Cellpose 1.0 to segment the same image in multiple ways. For this we had to build multiple models with different segmentation styles, i.e.:\n\n\n\n\n\nAll these segmentation styles are available in Cellpose 2.0 at the click of a button. Try them out!\n\n\n\n\n\nThis led us to think more carefully about personalized models for everyone. The main challenge is that deep learning typically requires a lot of training data…\nExcept it doesn’t. Not necessarily. When we initialize with Cellpose 1.0, a new model can be trained with ~500 segmented ROIs. The gains beyond that are minimal.\n\n\n\n\n\nAnd we can further reduce the training data requirement to 100-200 ROIs simply with a human-in-the-loop approach, where the user fixes the mistake of the algorithm instead of segmenting from scratch.\n\n\n\n\n\nAs the user segments more, new models are trained that require even fewer corrections. Within a few iterations, very competitive models can be trained, which match human performance (see blue curve below):\n\n\n\n\n\nHere’s our human-in-the-loop pipeline in action. We start by correcting the mistakes of Cellpose 1.0 (2x speedup)\n\n\n\n\n\nThen we train a new model on the single image we just segmented:\n\n\n\n\n\nAfter ~30 minutes, the user trains the fifth model, and this provides great results on various new images:\n\n\n\n\n\nSee the entire 30 minute procedure on youtube.\nFinal reveal: I [Marius] was the “human-in-the-loop” for all experiments! It’s actually quite fun. Try it out and please send us more training data via upload in the GUI. code: https://github.com/MouseLand/cellpose\n\n\nThe End.\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/unsupervised.html",
    "href": "research/posts/unsupervised.html",
    "title": "Unsupervised pretraining in biological neural networks",
    "section": "",
    "text": "Abstract\n\n\n\nRepresentation learning in neural networks may be implemented with supervised or unsupervised algorithms, distinguished by the availability of instruction. In the sensory cortex, perceptual learning drives neural plasticity, but it is not known whether this is due to supervised or unsupervised learning. Here we recorded populations of up to 90,000 neurons simultaneously from the primary visual cortex (V1) and higher visual areas (HVAs) while mice learned multiple tasks, as well as during unrewarded exposure to the same stimuli. Similar to previous studies, we found that neural changes in task mice were correlated with their behavioural learning. However, the neural changes were mostly replicated in mice with unrewarded exposure, suggesting that the changes were in fact due to unsupervised learning. The neural plasticity was highest in the medial HVAs and obeyed visual, rather than spatial, learning rules. In task mice only, we found a ramping reward-prediction signal in anterior HVAs, potentially involved in supervised learning. Our neural results predict that unsupervised learning may accelerate subsequent task learning, a prediction that we validated with behavioural experiments.\n\n\npaper | news article | dataset | code | preprint |\n\nThread by Lin Zhong:\n\nSimple question: How do we learn? Answer: From teachers (supervised).\n\nSure! But we also learn a lot on our own (unsupervised), and so do our mice.\n\n\n\n\n\nWe developed a virtual reality (VR) task in which mice discriminated textures in order to get reward (supervised cohort) OR they just ran for FUN (unsupervised cohort).\n\n\n\nMice learned to discriminate textures by licking in the corridor with reward.\n\n\n\n\nWe recorded up to 90,000 neurons from the visual cortex during learning to try to understand the neural mechanism. Neural activities are visualized using our sorting algorithm Rastermap with behavioral annotations.\n\n\n\n\nWe found that the plasticity in medial visual areas was mediated by unsupervised learning.\n\n\n\nMice correctly generalized the reward rule to new stimuli based on the visual similarities, behaviorally and neurally.\n\n\n\n\nMice learned to discriminate two very similar textures (leaf1 vs leaf2) by orthogonalizing them in the neural space\n\n\n\n\nLearning that only leaf1 was rewarded results in de-orthogonalization of another new leaf (leaf3).\n\n\n\n\nQuestion: Wait! We don’t need supervised learning at all? I will say no to my supervisor if that is true 🙃.\nOur results suggest I should think twice before doing that: inside the anterior visual areas we found a representation only in the supervised learning task, which can predict the reward and was highly correlated with behavior.\n\n\n\n\nQuestion: What does unsupervised learning do? One possible answer is to pre-train our neural network for subsequent tasks. Indeed, we show that mice learned much faster after experiencing unsupervised pretraining!\n\n\n\n\nWhat is more, we found that V1 and lateral visual areas can encode novelty when seeing a new stimulus after learning. The novelty responses went away after mice got familiar with the new stimulus.\n\n\n\n\nOur results show:\n\n\nMost learning is through unsupervised learning, mediated by medial visual areas\nSupervised learning may require anterior visual areas\nA third stream (V1 + lateral) encodes novelty in both supervised and unsupervised learning\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/highdim.html",
    "href": "research/posts/highdim.html",
    "title": "High-dimensional geometry of population responses in visual cortex",
    "section": "",
    "text": "Abstract\n\n\n\nA neuronal population encodes information most efficiently when its stimulus responses are high-dimensional and uncorrelated, and most robustly when they are lower-dimensional and correlated. Here we analysed the dimensionality of the encoding of natural images by large populations of neurons in the visual cortex of awake mice. The evoked population activity was high-dimensional, and correlations obeyed an unexpected power law: the nth principal component variance scaled as 1/n. This scaling was not inherited from the power law spectrum of natural images, because it persisted after stimulus whitening. We proved mathematically that if the variance spectrum was to decay more slowly then the population code could not be smooth, allowing small changes in input to dominate population activity. The theory also predicts larger power-law exponents for lower-dimensional stimulus ensembles, which we validated experimentally. These results suggest that coding smoothness may represent a fundamental constraint that determines correlations in neural population codes.\n\n\npaper | talk | Quanta news article | Simons news article | preprint | data | code | original tweeprint\n\n\n\nA picture is worth a thousand words, and your brain needs billions of neurons to process it. Why do we need so many neurons? To find out, we recorded thousands of them in mouse visual cortex.\n\n\n\n\n\nOne reason to have so many neurons may be that they each have different jobs:\n\nNeuron A recognizes the pointedness of a fox’s ears, Neuron B recognizes the color of the fox’s fur.  Neuron C recognizes a fox nose,  etc\n\n\n\n\nWhen enough of these neurons activate, the brain as a whole can recognize a fox.\n\n\n\n\nWhat if some neurons “fall asleep” on the job and don’t respond to the image? This actually happens very often, and yet the brain is remarkably robust to these failures.\n\nEven if 90% of the neurons don’t do their job, we can still recognize the fox. Even if we randomly change 90% of the pixels, we can still recognize the fox. The brain is robust to a lot of manipulations like that.\n\n\n\n\nArtificial neural networks also use millions of neurons to recognize images.\n\n\n\n\nUnlike brains, machines are not so robust to small aberrations. Here is our fox and next to it the same fox very slightly modified and now the machine thinks it’s a puffer fish!\n\n\n\n\nThese are called “adversarial images”, because we devised them to fool the machine. How does the brain protect against these perturbations and others?\nOne protection could be to make many slightly different copies of the neurons that represent foxes. Even if some neurons fall asleep on the job, their copies might still activate.\n\nHowever, if the brain used so many neurons for every single image, we would quickly run out of neurons!\n\nThis results in an evolutionary pressure: it’s good to have many neurons do very different jobs so we can recognize lots of objects in images, but it’s also good if they share some responsibilities, so they can pick up the slack when necessary.\n\nWe found evidence for this by investigating the main dimensions of variation in the responses of 10,000 neurons. Below, each column is one neuron’s responses to several of our images.\n\n\n\n\nThe largest two dimensions were distributed broadly across all neurons, as you see below. Any neuron could contribute to these and pick up the slack if the other neurons did not respond.\n\n\n\n\nThe next 8 dimensions each were smaller and distributed more sparsely across neurons. If a neuron was asleep, it was still likely a few others could represent these dimensions in its place.\n\n\n\n\nThe next 30 dimensions revealed ever more intricate structure…\n\n\n\n\nAnd so did the next 160 dimensions…\n\n\n\n\nAnd so on, this kept on going, with the N-th dimension being about N times smaller than the biggest dimension.\n\nThis distribution of activity is called a “power-law”.\n\n\n\n\nHowever, this was not just any power-law, it had a special exponent of approx 1. We did some math and showed that a power-law with this exponent must be borderline fractal.\n\nA fractal is a mathematical object that has structure at many different spatial scales, like the Mandelbrot set below:\n\n\n\n\n\nThis Inceptionism movie is also a kind of fractal:\n\n\n\n\n\nThe neural activity was so close to being a fractal, and just barely avoided it because it’s exponent was 1.04, not 1 or smaller.\n\nAn exponent of 1.04 is the sweet spot: as high-dimensional as possible without being a fractal.\n\nNot being a fractal allows neural responses to be continuous and smooth, which are the minimal protections neurons need so that we don’t confuse a fox with a puffer fish!\nWe shared the data, and the code to run the analyses. End of story, for now. data: (figshare) code: (github.com/MouseLand/stringer-pachitariu-et-al-2018b)\n\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "A simplified minimodel of visual cortical neurons\n\n\n\nVision\n\nThousands of neurons\n\nMachine learning\n\n\n\nSimplified and interpretable “minimodels” are sufficient to explain complex visual responses in mouse and monkey V1.\n\n\n\n\n\nFengtong Du, Miguel Angel Núñez-Ochoa, Marius Pachitariu†, Carsen Stringer†\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised pretraining in biological neural networks\n\n\n\nVision\n\nThousands of neurons\n\nBehavior\n\n\n\nUnsupervised learning - exposure to stimuli without rewards - drives large changes in neural activity in visual cortex, particularly in higher order medial visual areas.\n\n\n\n\n\nLin Zhong, Scott Baptista, Rachel Gattoni, Jon Arnold, Daniel Flickinger, Carsen Stringer†, Marius Pachitariu†\n\n\n\n\n\n\n\n\n\n\n\n\nCellpose3: one-click image restoration for improved cellular segmentation\n\n\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nSoftware for image restoration that works across many cell types and imaging modalities.\n\n\n\n\n\nCarsen Stringer, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nA critical initialization for biological neural networks\n\n\n\nThousands of neurons\n\nNeural modeling\n\n\n\nNeural recordings resemble linear dynamical systems governed by a symmetric connectivity matrix.\n\n\n\n\n\nMarius Pachitariu, Lin Zhong, Alexa Gracias, Amanda Minisi, Crystall Lopez, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nRastermap: A Discovery Method for Neural Population Recordings\n\n\n\nThousands of neurons\n\nMachine learning\n\nTools\n\n\n\nAnalysis tool for large-scale neural data which allows users to explore dynamical and spatial relationships among neurons.\n\n\n\n\n\nCarsen Stringer, Lin Zhong, Atika Syeda, Fengtong Du, Maria Kesa, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nSpike sorting with Kilosort4\n\n\n\nMachine learning\n\nTools\n\n\n\nAnalysis tool for accurate spike sorting, which is the computational process of extracting the firing times of single neurons from recordings of local electrical fields.\n\n\n\n\n\nMarius Pachitariu, Shaswat Sridhar, Jacob Pennington, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nFacemap: a framework for modeling neural activity based on orofacial tracking\n\n\n\nBehavior\n\nThousands of neurons\n\nMachine learning\n\nTools\n\n\n\nAnalysis tool for tracking mouse face keypoints and relating them to large-scale neural activity, using convolutional neural networks.\n\n\n\n\n\nAtika Syeda, Lin Zhong, Renee Tung, Will Long, Marius Pachitariu†, Carsen Stringer†\n\n\n\n\n\n\n\n\n\n\n\n\nCellpose 2.0: how to train your own model\n\n\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nSoftware for users to quickly and easily create accurate segmentation models for their own data.\n\n\n\n\n\nMarius Pachitariu, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nNot so spontaneous: Multi-dimensional representations of behaviors and context in sensory areas\n\n\n\nThousands of neurons\n\nBehavior\n\n\n\nSpontaneous activity across species is related to motor movements. These motor-driven patterns emerge during development, diverging from sensory-driven activity.\n\n\n\n\n\nLilach Avitan, Carsen Stringer\n\n\n\n\n\n\n\n\n\n\n\n\nHigh precision coding in visual cortex\n\n\n\nVision\n\nThousands of neurons\n\n\n\nVisual cortex encodes stimuli highly precisely, far surpassing behavioral precision in mice and humans. In a task, visual cortex does not contribute to behavioral variability.\n\n\n\n\n\nCarsen Stringer, Michalis Michaelos, Dmitri Tsyboulski, Sarah E. Lindo, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nCellpose: a generalist algorithm for cellular segmentation\n\n\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nAccurate deep learning-based cellular segmentation tool that works for a wide variety of images, and includes an easy to use GUI.\n\n\n\n\n\nCarsen Stringer, Tim Wang, Michalis Michaelos, Marius Pachitariu\n\n\n\n\n\n\n\n\n\n\n\n\nHigh-dimensional geometry of population responses in visual cortex\n\n\n\nVision\n\nThousands of neurons\n\n\n\nNeural population activity in response to natural images is high-dimensional, and the activity correlations obey a power law of 1/n.\n\n\n\n\n\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Matteo Carandini†, Kenneth D Harris†\n\n\n\n\n\n\n\n\n\n\n\n\nSpontaneous behaviors drive multidimensional, brainwide activity\n\n\n\nThousands of neurons\n\nBehavior\n\nMachine learning\n\n\n\nOngoing neural activity is high-dimensional and driven by the mouse’s behavior. These behavioral representations are orthogonal to stimulus-driven responses.\n\n\n\n\n\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Charu Bai Reddy, Matteo Carandini†, Kenneth D Harris†\n\n\n\n\n\n\n\n\n\n\n\n\nSuite2p: beyond 10,000 neurons with standard two-photon microscopy\n\n\n\nThousands of neurons\n\nSegmentation\n\nMachine learning\n\nTools\n\n\n\nSuite2p enables fast and accurate processing of large-scale neural recordings.\n\n\n\n\n\nMarius Pachitariu, Carsen Stringer, Sylvia Schröder, Mario Dipoppa, L. Federico Rossi, Matteo Carandini, Kenneth D. Harris\n\n\n\n\n\n\n\n\n\n\n\n\nInhibitory control of correlated intrinsic variability in cortical networks\n\n\n\nNeural modeling\n\nBehavior\n\n\n\nCortical networks exhibit large-scale fluctuations which creates noise correlations that impact sensory coding. Modeling and data analysis show that inhibition can control these fluctuations.\n\n\n\n\n\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Michael Okun, Peter Bartho, Kenneth D Harris, Maneesh Sahani, Nicholas Lesica\n\n\n\n\n\nNo matching items\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "positions.html",
    "href": "positions.html",
    "title": "Positions",
    "section": "",
    "text": "We are seeking postdocs to investigate how large populations of neurons perform complex computations. We collect and analyze recordings of 50,000+ neurons, developing machine learning tools to extract computational principles from these large-scale datasets. Please reach out to Carsen Stringer (stringerc@hhmi.org) with your CV if you are interested. We are looking for candidates with experience performing imaging experiments in animals and ideally some experience analyzing large-scale neural datasets. Please share a link to your github profile on your CV when applying, especially if you are coming from a computational background.\nAll research is internally funded by the Howard Hughes Medical Institute with highly competitive benefits. A first-year postdoc is compensated at a rate of $74,200.00 annually. For information about Janelia, please visit www.janelia.org/about-us. For more information about our neuroscience department, please visit www.janelia.org/our-research/mechanistic-cognitive-neuroscience. For more information about being at Janelia, please check out these videos. Janelia provides incredible support for scientific research (surgical support, animal behavior training, and optical engineering), enabling researchers to focus on their specific scientific questions.\nDiversity, equity and inclusion are important values at Janelia, and applicants should be dedicated to ensuring kindness and inclusion in their interactions with the scientific community and with other employees at Janelia."
  },
  {
    "objectID": "positions.html#postdocs",
    "href": "positions.html#postdocs",
    "title": "Positions",
    "section": "",
    "text": "We are seeking postdocs to investigate how large populations of neurons perform complex computations. We collect and analyze recordings of 50,000+ neurons, developing machine learning tools to extract computational principles from these large-scale datasets. Please reach out to Carsen Stringer (stringerc@hhmi.org) with your CV if you are interested. We are looking for candidates with experience performing imaging experiments in animals and ideally some experience analyzing large-scale neural datasets. Please share a link to your github profile on your CV when applying, especially if you are coming from a computational background.\nAll research is internally funded by the Howard Hughes Medical Institute with highly competitive benefits. A first-year postdoc is compensated at a rate of $74,200.00 annually. For information about Janelia, please visit www.janelia.org/about-us. For more information about our neuroscience department, please visit www.janelia.org/our-research/mechanistic-cognitive-neuroscience. For more information about being at Janelia, please check out these videos. Janelia provides incredible support for scientific research (surgical support, animal behavior training, and optical engineering), enabling researchers to focus on their specific scientific questions.\nDiversity, equity and inclusion are important values at Janelia, and applicants should be dedicated to ensuring kindness and inclusion in their interactions with the scientific community and with other employees at Janelia."
  },
  {
    "objectID": "people/index.html",
    "href": "people/index.html",
    "title": "People",
    "section": "",
    "text": "We are recruiting PhD students, see here for more info!"
  },
  {
    "objectID": "people/index.html#group-leaders",
    "href": "people/index.html#group-leaders",
    "title": "People",
    "section": "Group leaders",
    "text": "Group leaders\n\n\n\n\nMarius Pachitariu\n\n\nCarsen Stringer"
  },
  {
    "objectID": "people/index.html#postdocs",
    "href": "people/index.html#postdocs",
    "title": "People",
    "section": "Postdocs",
    "text": "Postdocs\n\n\n\n\nLin Zhong\n\n\nMiguel Angel Nunez\n\n\n\n\nQingqing Zhang"
  },
  {
    "objectID": "people/index.html#phd-students",
    "href": "people/index.html#phd-students",
    "title": "People",
    "section": "PhD students",
    "text": "PhD students\n\n\n\n\nFengtong Du\n\n\nAtika Syeda"
  },
  {
    "objectID": "people/index.html#affiliated-theory-fellows",
    "href": "people/index.html#affiliated-theory-fellows",
    "title": "People",
    "section": "Affiliated theory fellows",
    "text": "Affiliated theory fellows\n\n\n\n\nTosif Ahamed"
  },
  {
    "objectID": "people/index.html#software-engineers-full--and-part-time",
    "href": "people/index.html#software-engineers-full--and-part-time",
    "title": "People",
    "section": "Software engineers (full- and part-time)",
    "text": "Software engineers (full- and part-time)\n\n\n\n\n\nJacob Pennington\n\n\nChris Ki\n\n\n\n\nMichael Rariden"
  },
  {
    "objectID": "people/index.html#visitors-part-time",
    "href": "people/index.html#visitors-part-time",
    "title": "People",
    "section": "Visitors (part-time)",
    "text": "Visitors (part-time)\n\nMinh Thao Nguyen, PhD student, Novak lab\nOndrej Novak\nBeverly Seltzer, PhD student, Lewis lab\nLen Jacob, postdoc, Lewis lab\nLaura Lewis"
  },
  {
    "objectID": "people/index.html#alumni",
    "href": "people/index.html#alumni",
    "title": "People",
    "section": "Alumni",
    "text": "Alumni\n\nWill Long, research technician → Princeton PhD\nRenee Tung, research technician and JUS summer student → Columbia PhD\nSonia Joseph, research technician → Mila PhD\nMichalis Michaelos, research technician → Janelia vivarium experimenter\nIfedayo Emmanuel Adeyefa-Olasupo, postdoc"
  },
  {
    "objectID": "people/carsen.html",
    "href": "people/carsen.html",
    "title": "Carsen Stringer",
    "section": "",
    "text": "Carsen Stringer is a group leader at HHMI Janelia Research Campus. The lab develops algorithms for understanding large-scale neural activity. In addition, the lab works on general segmentation algorithms for cellular data, which enable fast and accurate processing of ~50,000 neuron recordings.\nHere’s a biography of Carsen written by Daniela Cassataro for Stories of Women in Neuroscience (includes a podcast version). Carsen also talks about past and current work in this podcast with Paul Middlebrooks.\nstringerc@hhmi.org  google scholar  carsen-stringer  computingnature  computingnature"
  },
  {
    "objectID": "people/carsen.html#education",
    "href": "people/carsen.html#education",
    "title": "Carsen Stringer",
    "section": "Education",
    "text": "Education\nUniversity College London, Gatsby Unit | 2018 PhD Computational Neuroscience Advisors: Kenneth Harris & Matteo Carandini\nUniversity of Pittsburgh | 2013  BSc Applied Math & Physics Advisor: Jonathan Rubin"
  },
  {
    "objectID": "people/carsen.html#teaching",
    "href": "people/carsen.html#teaching",
    "title": "Carsen Stringer",
    "section": "Teaching",
    "text": "Teaching\n(links to coding material)\n\nBehavioral encoding models for large-scale population activity, JHU bootcamp, 2023\nDeep Learning for Microscopy Image Analysis (lecturer), MBL Woodshole, 2022, 2023\nImaging Structure & Function in the Nervous System (lecturer), CSHL, 2018, 2019, 2022, 2023\nCAJAL Course on Interacting with Neural Circuits (lecturer), Champalimaud, 2019, 2022, 2023\nModern Optical Microscopy for the Modern Biologist, guest lecturer, UC Berkeley, 2022, 2023\nProbabilistic Machine Learning Course (co-instructor), Janelia+JHU, 2022-2023\nLearning to use suite2p and kilosort2 (co-instructor), Janelia, 2019\nNeural Data Science (co-instructor) CSHL 2019\nBoard of directors, Neuromatch Academy, 2021-2023\nDeep Learning for comp neuro (organizer/lecturer) + TA organizer Neuromatch Academy, 2020\nMathematical methods for neuroscience and ML (co-organizer), Janelia, 2019\nMachine Learning: Dimensionality reduction, Janelia, 2018\nTheoretical Neuroscience TA, Gatsby, UCL, 2014"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pachitariu + Stringer lab",
    "section": "",
    "text": "Our lab is at HHMI Janelia Research Campus (near DC), a fully-funded non-profit research institution. Janelia just received a $500 million investment for AI research 🤖, more info here.\nWe are recruiting postdocs!"
  },
  {
    "objectID": "index.html#research",
    "href": "index.html#research",
    "title": "Pachitariu + Stringer lab",
    "section": "Research",
    "text": "Research\nOur lab combines machine learning / AI techniques and large-scale imaging to investigate plasticity rules and sensory representations in cortical circuits. Example recording of 50,000+ neurons simultaneously at 3Hz, using two-photon calcium imaging:\n\n\n\nLearn more about our research here and see all publications here. We share all the data generated by our studies, linked here. We also have developed several data processing packages for the bio/neuro community: cellpose, kilosort, suite2p, facemap, and rastermap."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Pachitariu + Stringer lab",
    "section": "News",
    "text": "News\n\nFengtong Du’s paper on minimodels of visual cortical neurons in mice and monkeys now published! News coverage\nLin Zhong’s paper on unsupervised learning now published! News coverage\nCellpose3 now published! News coverage\nAnalysis methods for large-scale neuronal recordings review published!\nCellpose development highlighted in a Nature technology feature!\nRastermap paper is now out here! News coverage\nCongrats to Fengtong Du et al, preprint now out: Towards a simplified model of primary visual cortex!\nKilosort4 paper is now out here!\nCongrats to Lin Zhong et al, preprint now out on Distinct streams for supervised and unsupervised learning in the visual cortex!\nFacemap was featured in a Nature Methods research highlight!"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Pachitariu + Stringer lab",
    "section": "Resources",
    "text": "Resources\n\nAbout HHMI Janelia Research Campus\nHHMI’s values\nBeing a postdoc at Janelia"
  },
  {
    "objectID": "index.html#talks",
    "href": "index.html#talks",
    "title": "Pachitariu + Stringer lab",
    "section": "Talks",
    "text": "Talks\n\nCellpose3 talk+tutorial\nRastermap tutorial\nFacemap tutorial\nKilosort 4 talk\nMaking sense of large-scale neural and behavioral data with Facemap and Rastermap talk\nCellpose 2.0: how to train your own model talk, talk+tutorial\nCellpose talk, talk+tutorial\nSuite2p talk\nKilosort 1, 2, 2.5 & 3 talk\nHigh precision coding in visual cortex short talk, long talk\nHigh-dimensional geometry of population responses in visual cortex talk\nHigh-dimensional problems in neuroscience talk\nSpontaneous behaviors drive multidimensional, brainwide activity talk"
  },
  {
    "objectID": "index.html#media-coverage",
    "href": "index.html#media-coverage",
    "title": "Pachitariu + Stringer lab",
    "section": "Media Coverage",
    "text": "Media Coverage\n\nNew computational model could help shed light on how we see, Howard Hughes Medical Institute\nDoing This Might Actually Make You Smarter, Vice\n“Zoning Out” Actually Helps You Learn? Data From Up To 90,000 Brain Cells Says So, IFL Science\nZoning out could be beneficial—and may actually help us learn faster, Howard Hughes Medical Institute\nNewest version of Cellpose can spot cell boundaries even in cloudy conditions, Howard Hughes Medical Institute\nNovel visualization method helps make sense of large neuronal activity datasets, Medical Xpress\nNew tool helps scientists spot patterns in mountains of data, Howard Huges Medical Institute\nSix tips for going public with your lab’s software, Nature\nJanelia scientists release state-of-the-art spike-sorting software Kilosort4, Howard Hughes Medical Institute\nPredicting neural activity from facial expressions, Nature Methods\nTiny faces, big expressions: Reading rodent faces, Penn NeuroKnow\nReading the mouse mind from its face: New tool decodes neural activity using facial movements, Howard Hughes Medical Institute\nNoisy solo neurons show consistency in groups, Howard Hughes Medical Institute\nHigh precision coding: How the visual cortex processes information about the world, Scientifica\nNew tool maps boundaries of diverse cells in microscope images, Howard Hughes Medical Institute\nPython power-up: new image tool visualizes complex data, Nature\nA Power Law Keeps the Brain’s Perceptions Balanced, Quanta magazine\nPower Law Discovery May Explain Why You Can See the Forest and the Trees, Simons Foundation\n‘Noise’ in the Brain Encodes Surprisingly Important Signals, Quanta magazine\nThinking on the Go: Why Does the Whole Brain Light Up for Just the Smallest Movements?, Simons Foundation\nprofile + podcast, Stories of Women in Neuroscience (WIN) by Daniela Cassataro\nUnderstanding 40,000 neurons, Brain Inspired podcast by Paul Middlebrooks"
  },
  {
    "objectID": "research/publications.html",
    "href": "research/publications.html",
    "title": "Publications",
    "section": "",
    "text": "preprints\nMarius Pachitariu, Michael Rariden, Carsen Stringer. Cellpose-SAM: superhuman generalization for cellular segmentation. bioRxiv, 2025.\nMarius Pachitariu, Lin Zhong, Alexa Gracias, Amanda Minisi, Crystall Lopez, Carsen Stringer. A critical initialization for biological neural networks. bioRxiv, 2025.\nJason A Keller, Iljung S Kwak, Alyssa K Stark, Marius Pachitariu, Kristin Branson, Joshua T Dudman. Cortical control of innate behavior from subcortical demonstration. bioRxiv, 2025.\nFengtong Du, Miguel Angel Núñez-Ochoa, Marius Pachitariu†, Carsen Stringer†. Towards a simplified model of primary visual cortex. bioRxiv, 2024.\nJudith Hoeller, Lin Zhong, Marius Pachitariu, Sandro Romani. Bridging tuning and invariance with equivariant neuronal representations. bioRxiv, 2024.\nCarsen Stringer, Marius Pachitariu. Benchmarking cellular segmentation methods against Cellpose. bioRxiv, 2024.\nYuki Miura, Ji-il Kim, Ovidiu Jurjut, Kevin W Kelley, Xiao Yang, Xiaoyu Chen, Mayuri Vijay Thete, Omer Revah, Bianxiao Cui, Marius Pachitariu, Sergiu P Pasca. Assembloid model to study loop circuits of the human nervous system. bioRxiv, 2024.\nLeandro PL Jacob, Sydney M Bailes, Stephanie D Williams, Carsen Stringer, Laura D Lewis. Distributed fMRI dynamics predict distinct EEG rhythms across sleep and wakefulness. bioRxiv, 2024.\nStefanos Stagkourakis, Giada Spigolon, Markus Marks, Michael Feyder, Joseph Kim, Pietro Perona, Marius Pachitariu, David J Anderson. Anatomically distributed neural representations of instincts in the hypothalamus. bioRxiv, 2023.\n\n\n2025\nLin Zhong, Scott Baptista, Rachel Gattoni, Jon Arnold, Daniel Flickinger, Carsen Stringer†, Marius Pachitariu†. Unsupervised pretraining in biological neural networks Nature, 2025.\nCarsen Stringer, Marius Pachitariu. Cellpose3: one-click image restoration for improved cellular segmentation. Nature Methods, 2025.\nXuyu Qian, Kyle Coleman, Shunzhou Jiang, Andrea J Kriz, Jack H Marciano, Chunyu Luo, Chunhui Cai, Monica Devi Manam, Emre Caglayan, Aoi Otani, Urmi Ghosh, Diane D Shao, Rebecca E Andersen, Jennifer E Neil, Robert Johnson, Alexandra LeFevre, Jonathan L Hecht, Michael B Miller, Liang Sun, Carsen Stringer, Mingyao Li, Christopher A Walsh. Spatial transcriptomics reveals human cortical layer and area specification. Nature, 2025.\nValentina Gandin, Jun Kim, Liang-Zhong Yang, Yumin Lian, Takashi Kawase, Amy Hu, Konrad Rokicki, Greg Fleishman, Paul Tillberg, Alejandro Aguilera Castrejon, Carsen Stringer, Stephan Preibisch, Zhe J Liu. Deep-Tissue Spatial Omics: Imaging Whole-Embryo Transcriptomics and Subcellular Structures at High Spatial Resolution. Science, 2025.\nWeinan Sun, Johan Winnubst, Maanasa Natrajan, Chongxi Lai, Koichiro Kajikawa, Michalis Michaelos, Rachel Gattoni, Carsen Stringer, Daniel Flickinger, James E Fitzgerald, Nelson Spruston. Learning produces a hippocampal cognitive map in the form of an orthogonalized state machine. Nature, 2025.\n\n\n2024\nCarsen Stringer, Marius Pachitariu. Analysis methods for large-scale neuronal recordings. Science, 2024.\nAtika Syeda, Lin Zhong, Renee Tung, Will Long, Marius Pachitariu†, Carsen Stringer†. Facemap: A Framework for Modeling Neural Activity Based on Orofacial Tracking. Nature Neuroscience, 2024.\nMarius Pachitariu, Shashwat Sridhar, Jacob Pennington, Carsen Stringer. Spike sorting with Kilosort4. Nature Methods, 2024.\nCarsen Stringer, Lin Zhong, Atika Syeda, Fengtong Du, Maria Kesa, Marius Pachitariu. Rastermap: A Discovery Method for Neural Population Recordings. Nature Neuroscience, 2024.\nHelen Farrants, Yichun Shuai, William C Lemon, Christian Monroy Hernandez, Shang Yang, Ronak Patel, Guanda Qiao, Michelle S Frei, Jonathan B Grimm, Timothy L Hanson, Filip Tomaska, Glenn C Turner, Carsen Stringer, Philipp J Keller, Abraham G Beyene, Yao Chen, Yajie Liang, Luke D Lavis, Eric R Schreiter. A modular chemigenetic calcium indicator for multiplexed in vivo functional imaging. Nature Methods, 2024.\n\n\n2023\nJonathan W. Lovelace, Jingrui Ma, Saurabh Yadav, Karishma Chhabria, Hanbing Shen, Zhengyuan Pang, Tianbo Qi, Ruchi Sehgal, Yunxiao Zhang, Tushar Bali, Thomas Vaissiere, Shawn Tan, Yuejia Liu, Gavin Rumbaugh, Li Ye, David Kleinfeld, Carsen Stringer, Vineet Augustine. Vagal sensory neurons mediate the Bezold–Jarisch reflex and induce syncope. Nature, 2023.\n\n\n2022\nMarius Pachitariu, Carsen Stringer. Cellpose 2.0: How to Train Your Own Model. Nature Methods 19, no. 12 (2022): 1634–41. Research Briefing: A Cellular Segmentation Algorithm with Fast Customization.\nLilach Avitan, Carsen Stringer. Not so Spontaneous: Multi-Dimensional Representations of Behaviors and Context in Sensory Areas. Neuron 110, no. 19 (2022): 3064–75.\nKevin J Cutler, Carsen Stringer, Teresa W Lo, Luca Rappez, Nicholas Stroustrup, S Brook Peterson, Paul A Wiggins, Joseph D Mougous. Omnipose: A High-Precision Morphology-Independent Solution for Bacterial Cell Segmentation. Nature Methods 19, no. 11 (2022): 1438–48.\nRichard J Gardner, Erik Hermansen, Marius Pachitariu, Yoram Burak, Nils A Baas, Benjamin A Dunn, May-Britt Moser, Edvard I Moser. Toroidal Topology of Population Activity in Grid Cells. Nature 602, no. 7895 (2022): 123–28.\nBiraj Pandey, Marius Pachitariu, Bingni W. Brunton, and Kameron Decker Harris. Structured Random Receptive Fields Enable Informative Sensory Encodings. PLoS Computational Biology 18, no. 10 (2022): e1010484.\nBernard t Hart, Titipat Achakulvisut, Ayoade Adeyemi, Athena Akrami, Bradly Alicea, Alicia Alonso-Andres, Diego Alzate-Correa, … Marius Pachitariu, … Carsen Stringer, et al. Neuromatch Academy: A 3-Week, Online Summer School in Computational Neuroscience. Journal of Open Source Education 5, no. 49 (2022): 118.\nEdward Zagha, Jeffrey C Erlich, Soohyun Lee, Gyorgy Lur, Daniel H O’Connor, Nicholas A Steinmetz, Carsen Stringer, Hongdian Yang. The Importance of Accounting for Movement When Relating Neuronal Activity to Sensory and Cognitive Processes. Journal of Neuroscience 42, no. 8 (2022): 1375–82.\n\n\n2021\nCarsen Stringer, Michalis Michaelos, Dmitri Tsyboulski, Sarah E Lindo, Marius Pachitariu. High-Precision Coding in Visual Cortex. Cell 184, no. 10 (2021): 2767–78.\nCarsen Stringer, Tim Wang, Michalis Michaelos, Marius Pachitariu. Cellpose: A Generalist Algorithm for Cellular Segmentation. Nature Methods 18, no. 1 (2021): 100–106.\nNicholas A Steinmetz*, Cagatay Aydin*, Anna Lebedeva*, Michael Okun*, Marius Pachitariu*, Marius Bauza, Maxime Beau, et al. Neuropixels 2.0: A Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings. Science 372, no. 6539 (2021): eabf4588.\nAnne-Kathrin Eiselt, Susu Chen, Jim Chen, Jon Arnold, Tahnbee Kim, Marius Pachitariu, Scott M Sternson. Hunger or Thirst State Uncertainty Is Resolved by Outcome Evaluation in Medial Prefrontal Cortex to Guide Decision-Making. Nature Neuroscience 24, no. 7 (2021): 907–12.\nKyu Hyun Lee, Yu-Li Ni, Jennifer Colonell, Bill Karsh, Jan Putzeys, Marius Pachitariu, Timothy D Harris, Markus Meister. Electrode Pooling Can Boost the Yield of Extracellular Recordings with Switchable Silicon Probes. Nature Communications 12, no. 1 (2021): 5245.\nJoshua H. Siegle, Xiaoxuan Jia, Séverine Durand, Sam Gale, Corbett Bennett, Nile Graddis, Greggory Heller, …, Marius Pachitariu, et al. Survey of Spiking in the Mouse Visual System Reveals Functional Hierarchy. Nature 592, no. 7852 (2021): 86–92.\nTara van Viegen, Athena Akrami, Kathryn Bonnen, Eric DeWitt, Alexandre Hyafil, Helena Ledmyr, Grace W Lindsay, … Carsen Stringer, … Marius Pachitariu, et al. Neuromatch Academy: Teaching Computational Neuroscience with Global Accessibility. Trends in Cognitive Sciences 25, no. 7 (2021): 535–38.\n\n\n2020\nSylvia Schröder, Nicholas A Steinmetz, Michael Krumin, Marius Pachitariu, Matteo Rizzi, Leon Lagnado, Kenneth D Harris, Matteo Carandini. Arousal Modulates Retinal Output. Neuron 107, no. 3 (2020): 487–95.\n\n\n2019\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Matteo Carandini†, Kenneth D Harris†. High-Dimensional Geometry of Population Responses in Visual Cortex. Nature, 2019, 1.\nCarsen Stringer*, Marius Pachitariu*, Nicholas Steinmetz, Charu Bai Reddy, Matteo Carandini†, Kenneth D Harris†. Spontaneous Behaviors Drive Multidimensional, Brainwide Activity. Science 364, no. 6437 (2019): 255.\nCarsen Stringer, Marius Pachitariu. Computational Processing of Neural Recordings from Calcium Imaging Data. Current Opinion in Neurobiology 55 (2019): 22–31.\nWilliam E. Allen, Michael Z. Chen, Nandini Pichamoorthy, Rebecca H. Tien, Marius Pachitariu, Liqun Luo, Karl Deisseroth. Thirst Regulates Motivated Behavior through Modulation of Brainwide Neural Population Dynamics. Science 364, no. 6437 (2019): eaav3932.\n\n\n2018\nMarius Pachitariu, Carsen Stringer, Kenneth D Harris. Robustness of Spike Deconvolution for Neuronal Calcium Imaging. Journal of Neuroscience 38, no. 37 (2018): 7976–85.\nPhilipp Berens, Jeremy Freeman, Thomas Deneux, Nikolay Chenkov, Thomas McColgan, Artur Speiser, Jakob H Macke, … Marius Pachitariu, et al. Community-Based Benchmarking Improves Spike Rate Inference from Two-Photon Calcium Imaging Data. PLoS Computational Biology 14, no. 5 (2018): e1006157.\nMario Dipoppa, Adam Ranson, Michael Krumin, Marius Pachitariu, Matteo Carandini, Kenneth D. Harris. Vision and Locomotion Shape the Interactions between Neuron Types in Mouse Visual Cortex. Neuron 98, no. 3 (2018): 602–15.\n\n\n2017\nJames J Jun, Nicholas A Steinmetz, Joshua H Siegle, Daniel J Denman, Marius Bauza, Brian Barbarits, Albert K Lee, …, Marius Pachitariu, et al. Fully Integrated Silicon Probes for High-Density Recording of Neural Activity. Nature 551, no. 7679 (2017): 232–36.\n\n\n2016\nMarius Pachitariu, Nicholas Steinmetz, Shabnam Kadir, Matteo Carandini, Kenneth D Harris. Kilosort: Realtime Spike-Sorting for Extracellular Electrophysiology with Hundreds of Channels. bioRxiv/Neurips, 2016.\nMarius Pachitariu, Carsen Stringer, Sylvia Schröder, Mario Dipoppa, L Federico Rossi, Matteo Carandini, Kenneth D Harris. Suite2p: Beyond 10,000 Neurons with Standard Two-Photon Microscopy. bioRxiv, 2016.\nCarsen Stringer*, Marius Pachitariu*, Nicholas A Steinmetz, Michael Okun, Peter Bartho, Kenneth D Harris, Maneesh Sahani, Nicholas A Lesica. Inhibitory Control of Correlated Intrinsic Variability in Cortical Networks. Elife 5 (2016): e19695.\n\n\n2015\nMarius Pachitariu, Dmitry R Lyamzin, Maneesh Sahani, Nicholas A Lesica. State-Dependent Population Coding in Primary Auditory Cortex. Journal of Neuroscience 35, no. 5 (2015): 2058–73.\nJasper Poort, Adil G. Khan, Marius Pachitariu, Abdellatif Nemri, Ivana Orsolic, Julija Krupic, Marius Bauza, et al. Learning Enhances Sensory and Multiple Non-Sensory Representations in Primary Visual Cortex. Neuron 86, no. 6 (2015): 1478–90.\n\n\n2014\nErnesto Suárez, Steven Lettieri, Metthew C Zwier, Carsen Stringer, Sundar Raman Subramanian, Lillian T Chong, Daniel M Zuckerman. Simultaneous Computation of Dynamical and Equilibrium Information Using a Weighted Ensemble of Trajectories. J. Chem. Theory Comput. (2014), 10, 7, 2658–2667.\n\n\n2013\nMarius Pachitariu, Adam M Packer, Noah Pettit, Henry Dalgleish, Michael Hausser, Maneesh Sahani. Extracting Regions of Interest from Biological Images with Convolutional Sparse Block Coding. In Advances in Neural Information Processing Systems, 1745–53, 2013.\nMarius Pachitariu, Biljana Petreska, Maneesh Sahani. Recurrent Linear Models of Simultaneously-Recorded Neural Populations. Advances in Neural Information Processing Systems 26 (2013).\nMarius Pachitariu, Maneesh Sahani. Regularization and Nonlinearities for Neural Language Models: When Are They Needed?. arXiv Preprint arXiv:1301. 5650, 2013.\n\n\n2012\nMarius Pachitariu, Maneesh Sahani. Learning Visual Motion in Recurrent Neural Networks. In Neurips, 1331–39, 2012.\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/facemap.html",
    "href": "research/posts/facemap.html",
    "title": "Facemap: a framework for modeling neural activity based on orofacial tracking",
    "section": "",
    "text": "Recent studies in mice have shown that orofacial behaviors drive a large fraction of neural activity across the brain. To understand the nature and function of these signals, we need better computational models to characterize the behaviors and relate them to neural activity. Here we developed Facemap, a framework consisting of a keypoint tracker and a deep neural network encoder for predicting neural activity. Our algorithm for tracking mouse orofacial behaviors was more accurate than existing pose estimation tools, while the processing speed was several times faster, making it a powerful tool for real-time experimental interventions. The Facemap tracker was easy to adapt to data from new labs, requiring as few as 10 annotated frames for near-optimal performance. We used the keypoints as inputs to a deep neural network which predicts the activity of ~50,000 simultaneously-recorded neurons and, in visual cortex, we doubled the amount of explained variance compared to previous methods. Using this model, we found that the neuronal activity clusters that were well predicted from behavior were more spatially spread out across cortex. We also found that the deep behavioral features from the model had stereotypical, sequential dynamics that were not reversible in time. In summary, Facemap provides a stepping stone toward understanding the function of the brain-wide neural signals and their relation to behavior.\n\narticle | preprint | code | Nature Methods highlight | news article | talk | tweeprint\n\n\n\n\nTutorial:\n\n\nGUI:\n \n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/rastermap.html",
    "href": "research/posts/rastermap.html",
    "title": "Rastermap: A Discovery Method for Neural Population Recordings",
    "section": "",
    "text": "Abstract\n\n\n\nNeurophysiology has long progressed through exploratory experiments and chance discoveries. Anecdotes abound of researchers listening to spikes in real time and noticing patterns of activity related to ongoing stimuli or behaviors. With the advent of large-scale recordings, such close observation of data has become difficult. To find patterns in large-scale neural data, we developed ‘Rastermap’, a visualization method that displays neurons as a raster plot after sorting them along a one-dimensional axis based on their activity patterns. We benchmarked Rastermap on realistic simulations and then used it to explore recordings of tens of thousands of neurons from mouse cortex during spontaneous, stimulus-evoked and task-evoked epochs. We also applied Rastermap to whole-brain zebrafish recordings; to wide-field imaging data; to electrophysiological recordings in rat hippocampus, monkey frontal cortex and various cortical and subcortical regions in mice; and to artificial neural networks. Finally, we illustrate high-dimensional scenarios where Rastermap and similar algorithms cannot be used effectively.\n\n\npaper | code | talk | slides | news article | preprint | original tweeprint\n\n\nMake your next discovery using #Rastermap, a visualization method for large-scale neural data.\n\n\n\n\n\nYou can explore your data in the #Rastermap graphical user interface:\n\n\n\n\n\nRastermap finds single-trial sequences of neural activity in a virtual reality experiment:\n\n\n\n\n\nRastermap finds movement-related structure in spontaneous activity in complete darkness:\n\n\n\n\n\nRastermap sorting of hippocampus data from Grosmark & Buzsaki, 2016:\n\n\n\n\n\nRastermap sorting of wholebrain zebrafish activity from Chen et al, 2018:\n\n\n\n\n\nRastermap sorting of neurons from Reinforcement Learning agents playing Atari games:\n\n\n\n\n\nLearn more on our github: https://github.com/MouseLand/rastermap. Rastermap is fast thanks to numpy, scipy, numba, and scikit-learn. The GUI is powered by pyqt and pyqtgraph, and supports npz, npy, mat and nwb ophys files.\nExcited to see new datasets explored with this! If you have issues, please post an issue on the Rastermap github: https://github.com/MouseLand/rastermap/issues.\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/inhibitory.html",
    "href": "research/posts/inhibitory.html",
    "title": "Inhibitory control of correlated intrinsic variability in cortical networks",
    "section": "",
    "text": "Abstract\n\n\n\nCortical networks exhibit intrinsic dynamics that drive coordinated, large-scale fluctuations across neuronal populations and create noise correlations that impact sensory coding. To investigate the network-level mechanisms that underlie these dynamics, we developed novel computational techniques to fit a deterministic spiking network model directly to multi-neuron recordings from different rodent species, sensory modalities, and behavioral states. The model generated correlated variability without external noise and accurately reproduced the diverse activity patterns in our recordings. Analysis of the model parameters suggested that differences in noise correlations across recordings were due primarily to differences in the strength of feedback inhibition. Further analysis of our recordings confirmed that putative inhibitory neurons were indeed more active during desynchronized cortical states with weak noise correlations. Our results demonstrate that network models with intrinsically-generated variability can accurately reproduce the activity patterns observed in multi-neuron recordings and suggest that inhibition modulates the interactions between intrinsic dynamics and sensory inputs to control the strength of noise correlations.\n\n\npaper\n\n\nOur brains contain billions of neurons, which are continually producing electrical signals to relay information around the brain. Yet most of our knowledge of how the brain works comes from studying the activity of one neuron at a time. Recently, studies of multiple neurons have shown that they tend to be active together in short bursts called “up” states, which are followed by periods in which they are less active called “down” states. When we are sleeping or under a general anesthetic, the neurons may be completely silent during down states, but when we are awake the difference in activity between the two states is usually less extreme. However, it is still not clear how the neurons generate these patterns of activity.\n\nTo address this question, we studied the activity of neurons in the brains of awake and anesthetized rats, mice and gerbils. The experiments recorded electrical activity from many neurons at the same time and found a wide range of different activity patterns:\n\n\n\n\n\n\nWe built a spiking neuronal network model which recapitulated the up and down states in the data:\n\n\n\n\n\n\nWe fit the model parameters directly to the neural activity, and reproduced the spiking patterns across brain states:\n\n\n\n\n\n\nWe found that increasing the strength of these inhibitory signals in the model decreased the fluctuations in electrical activity across entire areas of the brain. Further analysis of the experimental data supported the model’s predictions by showing that inhibitory neurons – which act to reduce electrical activity in other neurons – were more active when there were fewer fluctuations in activity across the brain, in both anesthetized and awake animals.\n\n\nIn particular, in awake mice, we observed decreases in noise correlations and increases in inhibitory activity during locomotion:\n\n\n\n\n\nCheck out the paper for more details.\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/spontaneous.html",
    "href": "research/posts/spontaneous.html",
    "title": "Spontaneous behaviors drive multidimensional, brainwide activity",
    "section": "",
    "text": "Abstract\n\n\n\nNeuronal populations in sensory cortex produce variable responses to sensory stimuli and exhibit intricate spontaneous activity even without external sensory input. Cortical variability and spontaneous activity have been variously proposed to represent random noise, recall of prior experience, or encoding of ongoing behavioral and cognitive variables. Recording more than 10,000 neurons in mouse visual cortex, we observed that spontaneous activity reliably encoded a high-dimensional latent state, which was partially related to the mouse’s ongoing behavior and was represented not just in visual cortex but also across the forebrain. Sensory inputs did not interrupt this ongoing signal but added onto it a representation of external stimuli in orthogonal dimensions. Thus, visual cortical population activity, despite its apparently noisy structure, reliably encodes an orthogonal fusion of sensory and multidimensional behavioral information.\n\n\npaper | talk | Quanta news article | Simons news article | preprint | data | code | original tweeprint\n\n\n\n\nThread:\n\nNeurons in the brain are very chatty: they fire action potentials, their basic unit of communication, even when there is nothing to communicate. For example in this video, in visual cortex recordings in complete darkness. Lots of chatting:\n\n\n\n\n\nThe puzzle is that these neurons are in visual cortex, so they should only be talking about what the eyes tell them. But here they are in pitch black darkness, chatting continuously.\n\nThis has been known for a long time, ever since the first recordings of single neurons in live brains.\n\nNeuroscientists are divided. Some think the chattering is just noise. Others think the neurons are chatting about something very important, we just can’t understand their language.\n\nRecording one neuron at a time, we have no chance to understand this language. It’s like hearing only half of a conversation:\n\n\nNeuron B: “You’re just paranoid.” Neuron B: “He started it.”\n\n\nBut really more like only a millionth of it:\n\n\nNeuron B: “t”\n\nThe number of recorded neurons has improved dramatically. We can now pick up whole conversations:\n\n\nNeuron A: I think I saw something. Neuron B: You’re just paranoid. Neuron C: Shh, I’m trying to sleep here. Neuron B: He started it. Neuron D: I think we’re going to be eaten.\n\nWe recorded about 10,000 neurons and sorted their activity using an algorithm so you can see next to each other groups of neurons that talk about the same things:\n\n\n\n\nThis showed us there were many conversations going on at the same time. But we still didn’t know what the conversations were about.\n\nAnd for quite a while, we were clueless…\n\nBut then it hit us. It was movements! The neurons were talking about movements!\n\nThe neurons were following in real time the motor actions of the mouse, each of them chatting about a different type of movement!\n\nIn mice, these motor actions are things like running, whisking, grooming and sniffing:\n\n\n\n\n\nWe detected these motor actions from the videos using automated algorithms:\n\n\n\nAnd then used all this motor information to predict which neurons were chatting and when:\n\n\n\n\nWe predicted a lot, about 50% of the neural conversation. In fact, in every area of the brain we looked, we could predict about that much:\n\n\n\n\nTo conclude, neurons in mice, and probably in your brain, are always chatting about what you’re doing, even if these neurons normally respond to images or sounds or tickling.\n\nWhy are they having this conversation about motor actions? It’s a bit of a mystery. Our theory is that it helps the brain detect coincidences: when action X and stimulus Y happen at the same time.\n\nAction X + stimulus Y could often lead to a consequence Z. Z can be a juicy reward, or a dangerous situation.\n\nLuckily, there is a neuron in your brain somewhere who noticed the coincidence XY, so it can alert everyone about the possible consequence Z before it happens!\n\nThere are more esoteric interpretations, like the chattering being the mechanistic substrate of consciousness. We’ll leave this for others to ponder.\n\nWe shared the data, and the code to run the analyses. data: (figshare) code: (github.com/MouseLand/stringer-pachitariu-et-al-2018a)\n\n\n\nP.S. We followed up on this story here!\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/cellpose3.html",
    "href": "research/posts/cellpose3.html",
    "title": "Cellpose3: one-click image restoration for improved cellular segmentation",
    "section": "",
    "text": "Generalist methods for cellular segmentation have good out-of-the-box performance on a variety of image types; however, existing methods struggle for images that are degraded by noise, blurring or undersampling, all of which are common in microscopy. We focused the development of Cellpose3 on addressing these cases and here we demonstrate substantial out-of-the-box gains in segmentation and image quality for noisy, blurry and undersampled images. Unlike previous approaches that train models to restore pixel values, we trained Cellpose3 to output images that are well segmented by a generalist segmentation model, while maintaining perceptual similarity to the target images. Furthermore, we trained the restoration models on a large, varied collection of datasets, thus ensuring good generalization to user images. We provide these tools as ‘one-click’ buttons inside the graphical interface of Cellpose as well as in the Cellpose API.\n\npaper | talk+tutorial | slides | code | news coverage | preprint\n\nThis is an upgrade to Cellpose; if you’re unfamiliar with Cellpose, check it out here.\nDenoising, deblurring and upsampling examples:\n\n\n\nCellpose3 slides:\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/minimodels.html",
    "href": "research/posts/minimodels.html",
    "title": "A simplified minimodel of visual cortical neurons",
    "section": "",
    "text": "Abstract\n\n\n\nArtificial neural networks (ANNs) have been shown to predict neural responses in primary visual cortex (V1) better than classical models. However, this performance often comes at the expense of simplicity and interpretability. Here we introduce a new class of simplified ANN models that can predict over 70% of the response variance of V1 neurons. To achieve this high performance, we first recorded a new dataset of over 29,000 neurons responding to up to 65,000 natural image presentations in mouse V1. We found that ANN models required only two convolutional layers for good performance, with a relatively small first layer. We further found that we could make the second layer small without loss of performance, by fitting individual “minimodels” to each neuron. Similar simplifications applied for models of monkey V1 neurons. We show that the minimodels can be used to gain insight into how stimulus invariance arises in biological neurons.\n\n\npaper | code | tutorial | data | news coverage | preprint | original tweeprint\n\n\nThread by Fengtong Du:\n\nPredicting neural activity is notoriously difficult and requires complicated models. Here we develop simple “minimodels” which explain 70% of neural variance in V1! 🐭🐒\n\n\n\n\n\nWe started with population-level models, fitting all neurons together with 4 shared conv layers. These models performed better than past models because we showed many more images. The model predicted monkey V1 responses well too.\n\n\n\nBut we didn’t need such a deep network: two convolutional layers were sufficient, in both mice and monkeys. Also, the first layer could be very small, 16 filters, while the second layer did need to be large, in line with the high dimensionality of V1.\n\n\n\n\nThis structure – small first convolutional layer and large second convolutional layer – was advantageous for performing visual tasks, such as texture classification and image recognition.\n\n\n\n\nNext, can we simplify the wide second layer further? We found that using more neurons to fit the model did NOT help! This suggested that we could fit smaller models to individual neurons.\n\n\n\n\nSo we built a minimodel for each neuron, matching the performance of the best models. On average, mouse minimodels had 32 conv2 filters and monkey minimodels had 7, much fewer than the 320 filters in our previous model.\n\n\n\n\nNow equipped with a minimodel for each neuron, we used them to understand how the visual invariance of a single neuron develops across the model stages. We designed a metric, fraction of category variance (FECV) to measure this invariance.\n\n\n\n\nWe found that instead of gradually increasing, the invariance primarily emerges at the readout stage and is influenced by both pooling size and input channel similarity.\n\n\n\n\nWith these minimodels, we can also visualize the high and low FECV neurons in mouse and monkey V1.\n\n\n\n\nIn summary, we found single-neuron minimodels are just as powerful as larger ones! It offers an accurate and interpretable approach to studying visual computation across different species and experimental contexts. 🐭🐒\n\nHuge thanks to Janelia! Thanks to the GENIE project, the Vivarium staff, Sarah Lindo and Sal DiLisio for surgery, Jon Arnold for designing headbars and coverslips, Dan Flickinger for microscopy, and Jon Arnold and Tobias Goulet for engineering support.\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/notso.html",
    "href": "research/posts/notso.html",
    "title": "Not so spontaneous: Multi-dimensional representations of behaviors and context in sensory areas",
    "section": "",
    "text": "Sensory areas are spontaneously active in the absence of sensory stimuli. This spontaneous activity has long been studied; however, its functional role remains largely unknown. Recent advances in technology, allowing large-scale neural recordings in the awake and behaving animal, have transformed our understanding of spontaneous activity. Studies using these recordings have discovered high-dimensional spontaneous activity patterns, correlation between spontaneous activity and behavior, and dissimilarity between spontaneous and sensory-driven activity patterns. These findings are supported by evidence from developing animals, where a transition toward these characteristics is observed as the circuit matures, as well as by evidence from mature animals across species. These newly revealed characteristics call for the formulation of a new role for spontaneous activity in neural sensory computation.\n\npaper\n\n\n\n\nAcross development, dimensionality of spontaneous activity increases across development, becomes more related to neural activity, and orthogonal to stimulus-driven activity.\n\n\n\n\n\n\nBehavioral modulation and orthogonality of spontaneous and evoked patterns.\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  },
  {
    "objectID": "research/posts/highprecision.html",
    "href": "research/posts/highprecision.html",
    "title": "High precision coding in visual cortex",
    "section": "",
    "text": "Abstract\n\n\n\nIndividual neurons in visual cortex provide the brain with unreliable estimates of visual features. It is not known whether the single-neuron variability is correlated across large neural populations, thus impairing the global encoding of stimuli. We recorded simultaneously from up to 50,000 neurons in mouse primary visual cortex (V1) and in higher order visual areas and measured stimulus discrimination thresholds of 0.35° and 0.37°, respectively, in an orientation decoding task. These neural thresholds were almost 100 times smaller than the behavioral discrimination thresholds reported in mice. This discrepancy could not be explained by stimulus properties or arousal states. Furthermore, behavioral variability during a sensory discrimination task could not be explained by neural variability in V1. Instead, behavior-related neural activity arose dynamically across a network of non-sensory brain areas. These results imply that perceptual discrimination in mice is limited by downstream decoders, not by neural noise in sensory representations.\n\n\npaper | talk | HHMI news article | Scientifica article | preprint | data | code | original tweeprint\n\n\n\n\n\n\nThread:\n\nSingle neurons in the brain can’t be depended on for reliable information. Here are some neurons from our recent study, recorded twice in response to the same visual stimuli. Different neurons are active at different times!\n\n\n\n\nAsk a neuron what angle the corner of your screen makes and it will say 75 degrees right now, 100 degrees in 5 minutes, and some other random number close to 90 every time you ask.\n\n\n\nThat is not how a computational device should work! Imagine if your calculator gave different answers every time\n\n\n\n\nThis makes our lives as neuroscientists hard. Single measurements of neurons are not reliable (gray dots), and we need to repeat the measurements many times to average out the noise (black line).\n\n\n\n\nMaybe, we thought, the brain uses some kind of averaging over its millions of noisy neurons to get a clean estimate of what it’s looking at.\n\n\n\nIf that was true, there would be “magical” combinations of neurons, which averaged would give just the right answer. Can we find these “magical” combinations by looking at the brain while it’s looking at our images? We used a microscope to record the activity of ~20,000 neurons simultaneously. Here is all of them from one session in random colors.\n\n\n\n\nWe used linear regression to find weights for each neuron that combine their activities into “super-neurons”.\n\n\n\n\nThese super-neurons were much less noisy than single neurons. In fact, the super-neurons could tell the difference between 45 and 46 degrees on 95% of the test trials. Can you?\n\n\n\n\nImagine asking a mouse to distinguish such small differences… Our colleagues in @BenucciLab actually tried! The mouse could only tell apart angle differences of 29 degrees, which was about 100 times worse than the neurons.\n\n\n\n\nEven for humans it’s difficult, but I bet you can see the difference if I make the pictures into a movie.\n\n\n\n\nWe conclude that mice have a lot of information in their brains, which are 1000x smaller than ours.\n\n\n\n\nThey can’t communicate this information to us, but that does not mean they don’t use it, for example as a first step to another computation.\n\n\n\n\nWe hope to find out in the future what these other computations might be. We publicly shared the data and code from this paper if anyone wants to dig further. data: (figshare) code: (github.com/MouseLand/stringer-et-al-2019)\n\n\nThe End.\n\n\n\n\n\n\n\n  Powered by Quarto. © Marius Pachitariu & Carsen Stringer lab, 2023."
  }
]